{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openaikey = \"sk-proj-t8OP9gqe7Tus4yuC9AMdT3BlbkFJPRhshdXAlSAX0rhIXx9u\"\n",
    "\n",
    "# setting it to the environment variable\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = openaikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openaikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token,label,sentence_id\n",
      "I,O,0\n",
      "love,O,0\n",
      "using,O,0\n",
      "my,O,0\n",
      "iPhone,O,0\n",
      "!O,0\n",
      "Apple,B-company,0\n",
      "products,O,0\n",
      "are,O,0\n",
      "the,O,0\n",
      "best,O,0\n",
      ". O,0\n",
      "Just,O,1\n",
      "bought,O,1\n",
      "a,O,1\n",
      "new,O,1\n",
      "Macbook,B-product,1\n",
      "Pro,I-product,1\n",
      "!O,1\n",
      "Looking,O,1\n",
      "forward,O,1\n",
      "to,O,1\n",
      "using,O,1\n",
      "it,O,1\n",
      ". O,1\n",
      "Congratulations,O,2\n",
      "to,O,2\n",
      "Elon,B-person,2\n",
      "Musk,I-person,2\n",
      "and,O,2\n",
      "SpaceX,B-company,2\n",
      "for,O,2\n",
      "another,O,2\n",
      "successful,O,2\n",
      "rocket,O,2\n",
      "launch,O,2\n",
      ". O,2\n",
      "The,O,3\n",
      "Tesla,B-product,3\n",
      "Model,B-product,3\n",
      "3,I-product,3\n",
      "is,O,3\n",
      "amazing,O,3\n",
      "!O,3\n",
      "Can,O,3\n",
      "'t,O,3\n",
      "wait,O,3\n",
      "to,O,3\n",
      "drive,O,3\n",
      "it,O,3\n",
      ". O,3\n",
      "Google,B-company,4\n",
      "always,O,4\n",
      "comes,O,4\n",
      "up,O,4\n",
      "with,O,4\n",
      "innovative,O,4\n",
      "products,O,4\n",
      ". O,4\n",
      "Just,O,5\n",
      "booked,O,5\n",
      "tickets,O,5\n",
      "for,O,5\n",
      "the,O,5\n",
      "new,O,5\n",
      "Marvel,B-company,5\n",
      "movie,O,5\n",
      ". O,5\n",
      "Loving,O,5\n",
      "my,O,6\n",
      "new,O,6\n",
      "Asus,B-product,6\n",
      "laptop,O,6\n",
      "!O,6\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Create 1000 rows of synthetic tweet data for NER in CSV format.\n",
    "The file should consist of a row for every token in the tweets.\n",
    "Each row should include the following fields:\n",
    " - token\n",
    " - label\n",
    " - sentence_id\n",
    "\n",
    "The token-column contains only one token for each row, the tokens put together should form concise social media posts.\n",
    "The label represents the entity of every token. Only label companies, products and persons. Every other token gets the value \"O\".\n",
    "The sentence_id increments every time a new sentence begins. It starts at 0. \n",
    "\n",
    "\n",
    "Make sure that the tweets make sense. Also only respond with the data.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to generate synthetic data.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    ")\n",
    "res = response.choices[0].message.content\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "def generate_text(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  \n",
    "        prompt=prompt,\n",
    "        max_tokens=100  \n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "\n",
    "prompt = \"Generate a twitter post that is suited .\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./synthetic_tweet_data2.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "companies = ['Apple', 'Google', 'Microsoft', 'Tesla', 'Amazon', 'Samsung', 'Blackberry', 'Nokia', 'Sony', 'Nintendo', 'Sega']\n",
    "products = ['iPhone', 'iPad', 'Pixel', 'Windows', 'Model S', 'Echo', 'Galaxy', 'AirPods', 'PlayStation', 'Xbox', 'GameCube']\n",
    "persons = ['Elon', 'Musk', 'Tim', 'Cook', 'Steve Jobs', 'Jeff', 'Bezos', 'Sundar', 'Pichai', 'Smith', 'Miller', 'Davids']\n",
    "\n",
    "# Sample tweet templates\n",
    "tweet_templates = [\n",
    "    \"I love my new {product}, thanks {company}!\",\n",
    "    \"{person} just announced the new {product} from {company}.\",\n",
    "    \"The {product} from {company} is amazing!\",\n",
    "    \"{company}'s latest update to {product} is revolutionary.\",\n",
    "    \"{person} said that {company} is working on something big.\",\n",
    "    \"Have you seen the latest {company} event? {product} is the star!\",\n",
    "    \"{person} just bought a new {product}. Can't wait to try it!\",\n",
    "    \"Rumors say {company} is releasing {product} soon.\",\n",
    "    \"I hate the new {product}, the older ones are much better.\",\n",
    "    \"I just upgraded to the new {product}, {company} never disappoint!\",\n",
    "    \"{person} just hinted at new features in {company}'s upcoming {product}. I am hyped!\",\n",
    "    \"{person} is revolutionizing the industry with {company} and their {product}.\",\n",
    "    \"Just watched the {company} keynote. {product} looks impressive.\",\n",
    "    \"I'm curious, what do you think about {person}'s role at {company}?\",\n",
    "    \"Can not wait for {product} also. They should sell them down at SXSW.\",\n",
    "    \"The {company} store still has {product} and short lines.\",\n",
    "    \"more than 150 million mobile users for {product} for mobile #SXSW\",\n",
    "    \"Less than 2 hours until we announce the details on the {product} giveaway!\",\n",
    "    \"Is {company} going to drop another {product} soon? I need to know!\",\n",
    "    \"Looks like {person} is shaking things up at {company} with {product}.\",\n",
    "    \"{company} has a temporary Retail Store in Austin for the {product} release today. Opens at 5pm.\",\n",
    "    \"Reminder: {person} will be talking about {company} and {product} access today.\",\n",
    "    \"{company} is giving free {product} to open source coders who r attending this meet-up.\",\n",
    "    \"Is {person} still leading the innovation at {company} with {product}?\",\n",
    "    \"{company} just keeps raising the bar with every {product} they launch. Crazy!\",\n",
    "    \"{person} hyped up {company}'s {product}, but it's not that great in reality.\",\n",
    "    \"I am having so many issues with the {product}. {company} needs to fix this!\",\n",
    "    \"The innovation in {company}'s {product} is something only {person} could pull off.\",\n",
    "    \"Is it just me, or does {company}'s {product} feel rushed and unfinished?\",\n",
    "    \"Is it just me or does the new {product} feel rushed and unfinished? Disappointing\",\n",
    "    \"{company}'s {product} is overrated. I can't believe I fell for the marketing.\",\n",
    "    \"All eyes are on {company} after the announcement of their new {product}.\",\n",
    "    \"When {person} talks about {company}'s new {product}, you know it's going to be good.\",\n",
    "    \"The latest from {company}? Their {product} just dropped, and it's all over the internet.\",\n",
    "    \"Im amazed at how {person} transformed {company} with innovations like {product}.\",\n",
    "    \"The {product} is making me rethink my loyalty to {company}. Its not good.\",\n",
    "    \"Honestly, {person} needs to focus on fixing {company}'s {product} before releasing new ones.\",\n",
    "    \"{person} teased some big changes for {company}'s {product}. I wonder what's next.\",\n",
    "    \"I trusted {person} to deliver a great {product} at {company}, but this is a flop.\"\n",
    "    \"{company} just came out with a new model, the {product}, which is electric. I love that!\"\n",
    "]\n",
    "\n",
    "# Tokenize a sentence\n",
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "# Create the CSV data\n",
    "data = []\n",
    "sentence_id = 0\n",
    "\n",
    "while len(data) < 500000:\n",
    "    # Randomly select a template and replace placeholders\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "    \n",
    "    # Tokenize the tweet and assign labels\n",
    "    tokens = tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        label = \"O\"\n",
    "        if token == company:\n",
    "            label = \"B-company\"\n",
    "        elif token == product:\n",
    "            label = \"B-product\"\n",
    "        elif token in person.split():\n",
    "            label = \"B-person\"\n",
    "        data.append([token, label, sentence_id])\n",
    "    \n",
    "    sentence_id += 1\n",
    "\n",
    "# Write data to CSV\n",
    "csv_file_path = \"./synthetic_tweet_data2.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"token\", \"label\", \"sentence_id\"])\n",
    "    writer.writerows(data[:500000])\n",
    "\n",
    "csv_file_path\n",
    "\n",
    "# try out multiple versions of this function, with as many templates and elements as possible for diversity\n",
    "# experiment with sentence_id variations and entity-labelling variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./synthetic_tweet_data3.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenize a sentence with punctuation handling\n",
    "def tokenize(sentence):\n",
    "    # Split on spaces and punctuation\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "# Create the CSV data\n",
    "data = []\n",
    "sentence_id = 0\n",
    "\n",
    "while len(data) < 500000:\n",
    "    # Randomly select a template and replace placeholders\n",
    "    template = random.choice(tweet_templates)\n",
    "    company = random.choice(companies)\n",
    "    product = random.choice(products)\n",
    "    person = random.choice(persons)\n",
    "    tweet = template.format(company=company, product=product, person=person)\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    tokens = tokenize(tweet)\n",
    "    \n",
    "    # Iterate over tokens and build labels\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        label = \"O\"\n",
    "        \n",
    "        # Check for company, product, and person entities in token sequences\n",
    "        if ' '.join(tokens[i:i + len(company.split())]) == company:\n",
    "            for j in range(len(company.split())):\n",
    "                data.append([tokens[i + j], \"B-company\" if j == 0 else \"I-company\", sentence_id])\n",
    "            i += len(company.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(product.split())]) == product:\n",
    "            for j in range(len(product.split())):\n",
    "                data.append([tokens[i + j], \"B-product\" if j == 0 else \"I-product\", sentence_id])\n",
    "            i += len(product.split())\n",
    "            continue\n",
    "        elif ' '.join(tokens[i:i + len(person.split())]) == person:\n",
    "            for j in range(len(person.split())):\n",
    "                data.append([tokens[i + j], \"B-person\" if j == 0 else \"I-person\", sentence_id])\n",
    "            i += len(person.split())\n",
    "            continue\n",
    "        else:\n",
    "            data.append([token, label, sentence_id])\n",
    "            i += 1\n",
    "    \n",
    "    sentence_id += 1\n",
    "\n",
    "# Write data to CSV\n",
    "csv_file_path = \"./synthetic_tweet_data3.csv\"\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"token\", \"label\", \"sentence_id\"])\n",
    "    writer.writerows(data[:500000])\n",
    "\n",
    "csv_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
