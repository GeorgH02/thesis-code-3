{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df_tweets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./SA_data/twitter_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m df_tweets\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     ArrowDtype,\n\u001b[0;32m     65\u001b[0m     Int8Dtype,\n\u001b[0;32m     66\u001b[0m     Int16Dtype,\n\u001b[0;32m     67\u001b[0m     Int32Dtype,\n\u001b[0;32m     68\u001b[0m     Int64Dtype,\n\u001b[0;32m     69\u001b[0m     UInt8Dtype,\n\u001b[0;32m     70\u001b[0m     UInt16Dtype,\n\u001b[0;32m     71\u001b[0m     UInt32Dtype,\n\u001b[0;32m     72\u001b[0m     UInt64Dtype,\n\u001b[0;32m     73\u001b[0m     Float32Dtype,\n\u001b[0;32m     74\u001b[0m     Float64Dtype,\n\u001b[0;32m     75\u001b[0m     CategoricalDtype,\n\u001b[0;32m     76\u001b[0m     PeriodDtype,\n\u001b[0;32m     77\u001b[0m     IntervalDtype,\n\u001b[0;32m     78\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     79\u001b[0m     StringDtype,\n\u001b[0;32m     80\u001b[0m     BooleanDtype,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     NA,\n\u001b[0;32m     83\u001b[0m     isna,\n\u001b[0;32m     84\u001b[0m     isnull,\n\u001b[0;32m     85\u001b[0m     notna,\n\u001b[0;32m     86\u001b[0m     notnull,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     Index,\n\u001b[0;32m     89\u001b[0m     CategoricalIndex,\n\u001b[0;32m     90\u001b[0m     RangeIndex,\n\u001b[0;32m     91\u001b[0m     MultiIndex,\n\u001b[0;32m     92\u001b[0m     IntervalIndex,\n\u001b[0;32m     93\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     94\u001b[0m     DatetimeIndex,\n\u001b[0;32m     95\u001b[0m     PeriodIndex,\n\u001b[0;32m     96\u001b[0m     IndexSlice,\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     NaT,\n\u001b[0;32m     99\u001b[0m     Period,\n\u001b[0;32m    100\u001b[0m     period_range,\n\u001b[0;32m    101\u001b[0m     Timedelta,\n\u001b[0;32m    102\u001b[0m     timedelta_range,\n\u001b[0;32m    103\u001b[0m     Timestamp,\n\u001b[0;32m    104\u001b[0m     date_range,\n\u001b[0;32m    105\u001b[0m     bdate_range,\n\u001b[0;32m    106\u001b[0m     Interval,\n\u001b[0;32m    107\u001b[0m     interval_range,\n\u001b[0;32m    108\u001b[0m     DateOffset,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     to_numeric,\n\u001b[0;32m    111\u001b[0m     to_datetime,\n\u001b[0;32m    112\u001b[0m     to_timedelta,\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     Flags,\n\u001b[0;32m    115\u001b[0m     Grouper,\n\u001b[0;32m    116\u001b[0m     factorize,\n\u001b[0;32m    117\u001b[0m     unique,\n\u001b[0;32m    118\u001b[0m     value_counts,\n\u001b[0;32m    119\u001b[0m     NamedAgg,\n\u001b[0;32m    120\u001b[0m     array,\n\u001b[0;32m    121\u001b[0m     Categorical,\n\u001b[0;32m    122\u001b[0m     set_eng_float_format,\n\u001b[0;32m    123\u001b[0m     Series,\n\u001b[0;32m    124\u001b[0m     DataFrame,\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\api.py:9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m     12\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     13\u001b[0m     IntervalDtype,\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     isna,\n\u001b[0;32m     18\u001b[0m     isnull,\n\u001b[0;32m     19\u001b[0m     notna,\n\u001b[0;32m     20\u001b[0m     notnull,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     factorize,\n\u001b[0;32m     25\u001b[0m     unique,\n\u001b[0;32m     26\u001b[0m     value_counts,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py:50\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PerformanceWarning\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     51\u001b[0m     ExtensionDtype,\n\u001b[0;32m     52\u001b[0m     StorageExtensionDtype,\n\u001b[0;32m     53\u001b[0m     register_extension_dtype,\n\u001b[0;32m     54\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     56\u001b[0m     ABCCategoricalIndex,\n\u001b[0;32m     57\u001b[0m     ABCIndex,\n\u001b[0;32m     58\u001b[0m     ABCRangeIndex,\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     61\u001b[0m     is_bool,\n\u001b[0;32m     62\u001b[0m     is_list_like,\n\u001b[0;32m     63\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.read_csv(\"./SA_data/twitter_dataset.csv\")\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3142403"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  sentiment\n",
       "0           0  is upset that he can't update his Facebook by ...        0.0\n",
       "1           1  @Kenichan I dived many times for the ball. Man...        0.0\n",
       "2           2    my whole body feels itchy and like its on fire         0.0\n",
       "3           3  @nationwideclass no, it's not behaving at all....        0.0\n",
       "4           4                      @Kwesidei not the whole crew         0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_tweets[\"sentiment\"] = df_tweets[\"sentiment\"].replace({0.0 : \"negative\", 1.0 : \"positive\", 2.0 : \"neutral\"})\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0.0    1570067\n",
       "1.0    1561529\n",
       "2.0      10725\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sentiment.value_counts()\n",
    "# 0 = negative, 1 = positive, 2 = neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only taking a part of the data to save resources (with same label-splits as the whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following BERT-Pytorch SA tutorial from https://www.kaggle.com/code/chayan8/sentiment-analysis-using-bert-pytorch/notebook\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_tweets.index.values, \n",
    "                                                  df_tweets.sentiment.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=df_tweets.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"data_type\"] = \"not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.loc[X_train, \"data_type\"] = \"train\"\n",
    "df_tweets.loc[X_val, \"data_type\"] = \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>train</th>\n",
       "      <td>1334557</td>\n",
       "      <td>1334557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>235510</td>\n",
       "      <td>235510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>train</th>\n",
       "      <td>1327299</td>\n",
       "      <td>1327299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>234230</td>\n",
       "      <td>234230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2.0</th>\n",
       "      <th>train</th>\n",
       "      <td>9116</td>\n",
       "      <td>9116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1609</td>\n",
       "      <td>1609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0    tweet\n",
       "sentiment data_type                     \n",
       "0.0       train         1334557  1334557\n",
       "          val            235510   235510\n",
       "1.0       train         1327299  1327299\n",
       "          val            234230   234230\n",
       "2.0       train            9116     9116\n",
       "          val              1609     1609"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.groupby([\"sentiment\", \"data_type\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hausb\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m encoded_data_train \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_tweets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_tweets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtweet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m encoded_data_val \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m     13\u001b[0m     df_tweets[df_tweets\u001b[38;5;241m.\u001b[39mdata_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtweet\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m     14\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m input_ids_train \u001b[38;5;241m=\u001b[39m encoded_data_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3338\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3328\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3329\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3330\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3331\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3335\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3336\u001b[0m )\n\u001b[1;32m-> 3338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3340\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3356\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:882\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 882\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    884\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:849\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 849\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:695\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:156\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    154\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    161\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:356\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip_accents:\n\u001b[0;32m    355\u001b[0m             token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_strip_accents(token)\n\u001b[1;32m--> 356\u001b[0m     split_tokens\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_split_on_punc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    358\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m whitespace_tokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(split_tokens))\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tokens\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:382\u001b[0m, in \u001b[0;36mBasicTokenizer._run_split_on_punc\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(chars):\n\u001b[0;32m    381\u001b[0m     char \u001b[38;5;241m=\u001b[39m chars[i]\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_punctuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    383\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend([char])\n\u001b[0;32m    384\u001b[0m         start_new_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils.py:364\u001b[0m, in \u001b[0;36m_is_punctuation\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_punctuation\u001b[39m(char):\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether `char` is a punctuation character.\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_tweets[df_tweets.data_type==\"train\"].tweet.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df_tweets[df_tweets.data_type==\"val\"].tweet.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train[\"input_ids\"]\n",
    "attention_masks_train = encoded_data_train[\"attention_mask\"]\n",
    "labels_train = torch.tensor(df_tweets[df_tweets.data_type==\"train\"].sentiment.values)\n",
    "\n",
    "input_ids_val = encoded_data_val[\"input_ids\"]\n",
    "attention_masks_val = encoded_data_val[\"attention_mask\"]\n",
    "labels_val = torch.tensor(df_tweets[df_tweets.data_type==\"val\"].sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, \n",
    "                              attention_masks_train,\n",
    "                              labels_train)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, \n",
    "                            attention_masks_val,\n",
    "                           labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                      \"bert-base-uncased\", \n",
    "                                      num_labels = 3,\n",
    "                                      output_attentions = False,\n",
    "                                      output_hidden_states = False\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=RandomSampler(dataset_train),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    sampler=RandomSampler(dataset_val),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 1e-5,\n",
    "    eps = 1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps = len(dataloader_train)*epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 12\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc='Epoch {:1d}'.format(epoch), \n",
    "                        leave=False, \n",
    "                        disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        loss_train_total +=loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n",
    "    \n",
    "    #torch.save(model.state_dict(), f'Models/BERT_ft_Epoch{epoch}.model')\n",
    "    \n",
    "    tqdm.write('\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# following tutorial https://github.com/nicknochnack/BERTSentiment/blob/main/Sentiment.ipynb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hausb\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode('It was good but couldve been better. Great', return_tensors='pt')\n",
    "result = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7768, -1.2353,  1.4419,  1.9804,  0.4584]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BerTweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bertweet-sentiment analysis \n",
    "# https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sentiment_score(text, tokenizer, model):\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[{\"word\": \".\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>[{\"word\": \"\\u0120festival\", \"entity\": \"B-event...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[{\"word\": \"xt\", \"entity\": \"I-event\"}, {\"word\":...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1           1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2           2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3           3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4           4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                            entities  \n",
       "0  [{\"word\": \".\", \"entity\": \"B-person\"}, {\"word\":...  \n",
       "1  [{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...  \n",
       "2  [{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...  \n",
       "3  [{\"word\": \"\\u0120festival\", \"entity\": \"B-event...  \n",
       "4  [{\"word\": \"xt\", \"entity\": \"I-event\"}, {\"word\":...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df_brd_sa = pd.read_csv(\"./analysis_data/Brand Sentiment Analysis Dataset/Dataset - Train.csv\")\n",
    "\n",
    "# read in data after NER csv\n",
    "df_brd_sa = pd.read_csv(\"./data after NER.csv\")\n",
    "\n",
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"sentiment_prediction\"] = df_brd_sa[\"tweet_text\"].apply(lambda x: sentiment_score(x[:500], tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_brd_sa[\"sentiment_prediction\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hausb\\AppData\\Local\\Temp\\ipykernel_27056\\2089848440.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"] = df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace({\"Negative emotion\" : 0, \"Positive emotion\" : 2, \"No emotion toward brand or product\" : 1, \"I can't tell\" : 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>entities</th>\n",
       "      <th>sentiment_prediction_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"word\": \".\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>2</td>\n",
       "      <td>[{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>2</td>\n",
       "      <td>[{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"word\": \"\\u0120festival\", \"entity\": \"B-event...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>2</td>\n",
       "      <td>[{\"word\": \"xt\", \"entity\": \"I-event\"}, {\"word\":...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         tweet_text  \\\n",
       "0           0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1           1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2           2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3           3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4           4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                                  0    \n",
       "1                                                  2    \n",
       "2                                                  2    \n",
       "3                                                  0    \n",
       "4                                                  2    \n",
       "\n",
       "                                            entities  \\\n",
       "0  [{\"word\": \".\", \"entity\": \"B-person\"}, {\"word\":...   \n",
       "1  [{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...   \n",
       "2  [{\"word\": \"@\", \"entity\": \"B-person\"}, {\"word\":...   \n",
       "3  [{\"word\": \"\\u0120festival\", \"entity\": \"B-event...   \n",
       "4  [{\"word\": \"xt\", \"entity\": \"I-event\"}, {\"word\":...   \n",
       "\n",
       "   sentiment_prediction_finetuned  \n",
       "0                               0  \n",
       "1                               2  \n",
       "2                               2  \n",
       "3                               0  \n",
       "4                               2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 negative\n",
    "# 1 neutral\n",
    "# 2 positive\n",
    "df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"] = df_brd_sa[\"is_there_an_emotion_directed_at_a_brand_or_product\"].replace({\"Negative emotion\" : 0, \"Positive emotion\" : 2, \"No emotion toward brand or product\" : 1, \"I can't tell\" : 1})\n",
    "df_brd_sa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def performance_metrics(df, label, prediction):\n",
    "     accuracy = accuracy_score(df[label], df[prediction])\n",
    "     precision = precision_score(df[label], df[prediction], average=\"weighted\")\n",
    "     recall = recall_score(df[label], df[prediction], average=\"weighted\")\n",
    "     f1 = f1_score(df[label], df[prediction], average=\"weighted\")\n",
    "\n",
    "     print(f\"Accuracy: {accuracy}\")\n",
    "     print(f\"Precision: {precision}\")\n",
    "     print(f\"Recall: {recall}\")\n",
    "     print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7289577635976907\n",
      "Precision: 0.8810658835129812\n",
      "Recall: 0.7289577635976907\n",
      "F1-Score: 0.7884397587742731\n"
     ]
    }
   ],
   "source": [
    "performance_metrics(df_brd_sa, \"is_there_an_emotion_directed_at_a_brand_or_product\", \"sentiment_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa.to_csv(\"./data after SA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bertweet fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to fine-tune this model (goal of accuracy> and precision>)\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df_brd_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6611263183fe4b7c834a078f050ad023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example['tweet_text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column('is_there_an_emotion_directed_at_a_brand_or_product', 'labels')\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, # adjust depending on resources\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2, # adjust depending on resources\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hausb\\AppData\\Local\\Temp\\ipykernel_7652\\3252959399.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: hausbichler-georg (hausbichler-georg-wirtschaftsuniversit-t-wien). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\hausb\\OneDrive\\Desktop\\Uni\\Bachelorarbeit\\thesis-code-3\\wandb\\run-20241209_203015-66g733w1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface/runs/66g733w1' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface' target=\"_blank\">https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface/runs/66g733w1' target=\"_blank\">https://wandb.ai/hausbichler-georg-wirtschaftsuniversit-t-wien/huggingface/runs/66g733w1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e7b3d30fd44d47ab6c738c06f94875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4696, 'grad_norm': 34.91965103149414, 'learning_rate': 1.2401215805471124e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5a349495204dfcad58dfe1c11c44c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3217962682247162, 'eval_accuracy': 0.9044006069802731, 'eval_precision': 0.8776387724816433, 'eval_recall': 0.9044006069802731, 'eval_f1': 0.8906713195717271, 'eval_runtime': 125.3936, 'eval_samples_per_second': 5.255, 'eval_steps_per_second': 1.316, 'epoch': 1.0}\n",
      "{'loss': 0.3243, 'grad_norm': 56.448238372802734, 'learning_rate': 4.80243161094225e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d76347f69243c6a0245a178f167f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3860619068145752, 'eval_accuracy': 0.91350531107739, 'eval_precision': 0.8822575684694961, 'eval_recall': 0.91350531107739, 'eval_f1': 0.8969385263946141, 'eval_runtime': 115.0542, 'eval_samples_per_second': 5.728, 'eval_steps_per_second': 1.434, 'epoch': 2.0}\n",
      "{'train_runtime': 7890.3636, 'train_samples_per_second': 0.667, 'train_steps_per_second': 0.167, 'train_loss': 0.36438215539810503, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1316, training_loss=0.36438215539810503, metrics={'train_runtime': 7890.3636, 'train_samples_per_second': 0.667, 'train_steps_per_second': 0.167, 'total_flos': 346257257730048.0, 'train_loss': 0.36438215539810503, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d104a3286a40b8a62ec51f1e89cf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3217962682247162, 'eval_accuracy': 0.9044006069802731, 'eval_precision': 0.8776387724816433, 'eval_recall': 0.9044006069802731, 'eval_f1': 0.8906713195717271, 'eval_runtime': 114.8893, 'eval_samples_per_second': 5.736, 'eval_steps_per_second': 1.436, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/bertweetSA_after_inital_finetuning\\\\tokenizer_config.json',\n",
       " './models/bertweetSA_after_inital_finetuning\\\\special_tokens_map.json',\n",
       " './models/bertweetSA_after_inital_finetuning\\\\vocab.txt',\n",
       " './models/bertweetSA_after_inital_finetuning\\\\bpe.codes',\n",
       " './models/bertweetSA_after_inital_finetuning\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./models/bertweetSA_after_inital_finetuning')\n",
    "tokenizer.save_pretrained('./models/bertweetSA_after_inital_finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and trying the saved model\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"./models/bertweetSA_after_inital_finetuning\")\n",
    "\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"./models/bertweetSA_after_inital_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brd_sa[\"sentiment_prediction_finetuned\"] = df_brd_sa[\"tweet_text\"].apply(lambda x: sentiment_score(x[:500], tokenizer2, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9310240048617442\n",
      "Precision: 0.9037352697049685\n",
      "Recall: 0.9310240048617442\n",
      "F1-Score: 0.9170998038436116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hausb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# very high results, but may not be too telling because it was tested on same data it was finetuned on\n",
    "performance_metrics(df_brd_sa, \"is_there_an_emotion_directed_at_a_brand_or_product\", \"sentiment_prediction_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>file_name</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>group_name</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screenname</th>\n",
       "      <th>search_query</th>\n",
       "      <th>text</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>username</th>\n",
       "      <th>polarity</th>\n",
       "      <th>partition_0</th>\n",
       "      <th>partition_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-12 09:24:26</td>\n",
       "      <td>AMD</td>\n",
       "      <td>25</td>\n",
       "      <td>114</td>\n",
       "      <td>AMD</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>moffphcgaming</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Been on holiday so back now. Gonna try get som...</td>\n",
       "      <td>1.282244e+18</td>\n",
       "      <td>MoffPHC Gaming</td>\n",
       "      <td>-0.3102</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-12 09:09:36</td>\n",
       "      <td>AMD</td>\n",
       "      <td>159</td>\n",
       "      <td>1144</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalverse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ironparr0t</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @NinjaParanoid: #AMD #Ryzen #3900x #rtx2070...</td>\n",
       "      <td>1.282241e+18</td>\n",
       "      <td>ironparrot</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-12 08:37:31</td>\n",
       "      <td>AMD</td>\n",
       "      <td>4931</td>\n",
       "      <td>7</td>\n",
       "      <td>AMD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ASUS_ROG_IN</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>The Beast #StrixGA35 comes packed with Up to A...</td>\n",
       "      <td>1.282233e+18</td>\n",
       "      <td>ASUS ROG IN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-12 08:31:24</td>\n",
       "      <td>AMD</td>\n",
       "      <td>9</td>\n",
       "      <td>188</td>\n",
       "      <td>AMD</td>\n",
       "      <td>HAHA no....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>XApochrypha</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Recently purchased everything for my first per...</td>\n",
       "      <td>1.282231e+18</td>\n",
       "      <td>XenosApochrypha</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-12 08:16:45</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalocean</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @LinuxReviews: #Linux architect Linus Torva...</td>\n",
       "      <td>1.282227e+18</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at file_name  followers  friends group_name  \\\n",
       "0  2020-07-12 09:24:26       AMD         25      114        AMD   \n",
       "1  2020-07-12 09:09:36       AMD        159     1144        AMD   \n",
       "2  2020-07-12 08:37:31       AMD       4931        7        AMD   \n",
       "3  2020-07-12 08:31:24       AMD          9      188        AMD   \n",
       "4  2020-07-12 08:16:45       AMD       1719        1        AMD   \n",
       "\n",
       "         location  retweet_count     screenname search_query  \\\n",
       "0  United Kingdom            0.0  moffphcgaming         #AMD   \n",
       "1    digitalverse            4.0     ironparr0t         #AMD   \n",
       "2             NaN            0.0    ASUS_ROG_IN         #AMD   \n",
       "3     HAHA no....            0.0    XApochrypha         #AMD   \n",
       "4    digitalocean            1.0    LinuxDreams         #AMD   \n",
       "\n",
       "                                                text    twitter_id  \\\n",
       "0  Been on holiday so back now. Gonna try get som...  1.282244e+18   \n",
       "1  RT @NinjaParanoid: #AMD #Ryzen #3900x #rtx2070...  1.282241e+18   \n",
       "2  The Beast #StrixGA35 comes packed with Up to A...  1.282233e+18   \n",
       "3  Recently purchased everything for my first per...  1.282231e+18   \n",
       "4  RT @LinuxReviews: #Linux architect Linus Torva...  1.282227e+18   \n",
       "\n",
       "           username  polarity partition_0 partition_1  \n",
       "0  MoffPHC Gaming   -0.3102  Technology         AMD  \n",
       "1        ironparrot    0.0000  Technology         AMD  \n",
       "2       ASUS ROG IN    0.0000  Technology         AMD  \n",
       "3   XenosApochrypha    0.0000  Technology         AMD  \n",
       "4       LinuxDreams   -0.3612  Technology         AMD  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using big tech companies dataset to test \n",
    "# Big Tech Companies - Tweet Sentiment https://www.kaggle.com/datasets/wjia26/big-tech-companies-tweet-sentiment/data\n",
    "df_tweets_bigtech = pd.read_csv(\"./analysis_data/Big Tech Companies - Tweet Sentiment/Bigtech - 12-07-2020 till 19-09-2020/Bigtech - 12-07-2020 till 19-09-2020.csv\")\n",
    "df_tweets_bigtech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tweets_bigtech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing emojis from the tweets\n",
    "import regex as re\n",
    "df_tweets_bigtech[\"text\"] = df_tweets_bigtech[\"text\"].str.replace(\"[^A-Za-z0-9]\", \"\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9975 0.9968\n"
     ]
    }
   ],
   "source": [
    "# transforming polarity-column to be comparable with labels given by bertweet\n",
    "min_polarity = df_tweets_bigtech[\"polarity\"].min()\n",
    "max_polarity = df_tweets_bigtech[\"polarity\"].max()\n",
    "print(min_polarity, max_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 negative\n",
    "# 1 neutral\n",
    "# 2 positive\n",
    "# function converting polarity to sentiment\n",
    "def polarity_to_sentiment(polarity):\n",
    "    if -0.2 <= polarity <= 0.2:\n",
    "        return 1\n",
    "    if polarity > 0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>file_name</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>group_name</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screenname</th>\n",
       "      <th>search_query</th>\n",
       "      <th>text</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>username</th>\n",
       "      <th>polarity</th>\n",
       "      <th>partition_0</th>\n",
       "      <th>partition_1</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-12 09:24:26</td>\n",
       "      <td>AMD</td>\n",
       "      <td>25</td>\n",
       "      <td>114</td>\n",
       "      <td>AMD</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>moffphcgaming</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Been on holiday so back now. Gonna try get som...</td>\n",
       "      <td>1.282244e+18</td>\n",
       "      <td>MoffPHC Gaming</td>\n",
       "      <td>-0.3102</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-12 09:09:36</td>\n",
       "      <td>AMD</td>\n",
       "      <td>159</td>\n",
       "      <td>1144</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalverse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ironparr0t</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @NinjaParanoid: #AMD #Ryzen #3900x #rtx2070...</td>\n",
       "      <td>1.282241e+18</td>\n",
       "      <td>ironparrot</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-12 08:31:24</td>\n",
       "      <td>AMD</td>\n",
       "      <td>9</td>\n",
       "      <td>188</td>\n",
       "      <td>AMD</td>\n",
       "      <td>HAHA no....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>XApochrypha</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Recently purchased everything for my first per...</td>\n",
       "      <td>1.282231e+18</td>\n",
       "      <td>XenosApochrypha</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-12 08:16:45</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalocean</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @LinuxReviews: #Linux architect Linus Torva...</td>\n",
       "      <td>1.282227e+18</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-07-12 08:11:41</td>\n",
       "      <td>AMD</td>\n",
       "      <td>69</td>\n",
       "      <td>135</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>#Linux architect Linus Torvalds: AVX512 Is \"A ...</td>\n",
       "      <td>1.282226e+18</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at file_name  followers  friends group_name  \\\n",
       "0  2020-07-12 09:24:26       AMD         25      114        AMD   \n",
       "1  2020-07-12 09:09:36       AMD        159     1144        AMD   \n",
       "3  2020-07-12 08:31:24       AMD          9      188        AMD   \n",
       "4  2020-07-12 08:16:45       AMD       1719        1        AMD   \n",
       "7  2020-07-12 08:11:41       AMD         69      135        AMD   \n",
       "\n",
       "         location  retweet_count     screenname search_query  \\\n",
       "0  United Kingdom            0.0  moffphcgaming         #AMD   \n",
       "1    digitalverse            4.0     ironparr0t         #AMD   \n",
       "3     HAHA no....            0.0    XApochrypha         #AMD   \n",
       "4    digitalocean            1.0    LinuxDreams         #AMD   \n",
       "7       Amsterdam            1.0   LinuxReviews         #AMD   \n",
       "\n",
       "                                                text    twitter_id  \\\n",
       "0  Been on holiday so back now. Gonna try get som...  1.282244e+18   \n",
       "1  RT @NinjaParanoid: #AMD #Ryzen #3900x #rtx2070...  1.282241e+18   \n",
       "3  Recently purchased everything for my first per...  1.282231e+18   \n",
       "4  RT @LinuxReviews: #Linux architect Linus Torva...  1.282227e+18   \n",
       "7  #Linux architect Linus Torvalds: AVX512 Is \"A ...  1.282226e+18   \n",
       "\n",
       "           username  polarity partition_0 partition_1  sentiment  \n",
       "0  MoffPHC Gaming   -0.3102  Technology         AMD          0  \n",
       "1        ironparrot    0.0000  Technology         AMD          1  \n",
       "3   XenosApochrypha    0.0000  Technology         AMD          1  \n",
       "4       LinuxDreams   -0.3612  Technology         AMD          0  \n",
       "7      LinuxReviews   -0.3612  Technology         AMD          0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_bigtech[\"sentiment\"] = df_tweets_bigtech[\"polarity\"].apply(polarity_to_sentiment)\n",
    "df_tweets_bigtech.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    277033\n",
       "2    260709\n",
       "0     76243\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_bigtech[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>file_name</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>group_name</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screenname</th>\n",
       "      <th>search_query</th>\n",
       "      <th>text</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>username</th>\n",
       "      <th>polarity</th>\n",
       "      <th>partition_0</th>\n",
       "      <th>partition_1</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-12 09:24:26</td>\n",
       "      <td>AMD</td>\n",
       "      <td>25</td>\n",
       "      <td>114</td>\n",
       "      <td>AMD</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>moffphcgaming</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Been on holiday so back now. Gonna try get som...</td>\n",
       "      <td>1.282244e+18</td>\n",
       "      <td>MoffPHC Gaming</td>\n",
       "      <td>-0.3102</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-12 08:16:45</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalocean</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @LinuxReviews: #Linux architect Linus Torva...</td>\n",
       "      <td>1.282227e+18</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-12 08:11:41</td>\n",
       "      <td>AMD</td>\n",
       "      <td>69</td>\n",
       "      <td>135</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>#Linux architect Linus Torvalds: AVX512 Is \"A ...</td>\n",
       "      <td>1.282226e+18</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-12 02:22:50</td>\n",
       "      <td>AMD</td>\n",
       "      <td>34</td>\n",
       "      <td>155</td>\n",
       "      <td>AMD</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NdrewGarcia</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>#AMD stuck in a range box chart https://t.co/5...</td>\n",
       "      <td>1.282138e+18</td>\n",
       "      <td>Encino_Man</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-11 23:58:44</td>\n",
       "      <td>AMD</td>\n",
       "      <td>802</td>\n",
       "      <td>730</td>\n",
       "      <td>AMD</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Roger_Clinton1</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>$AMD Epyc Milan Leak  Three early Genesis sam...</td>\n",
       "      <td>1.282102e+18</td>\n",
       "      <td>Roger Ocasio-Clinton</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at file_name  followers  friends group_name  \\\n",
       "0  2020-07-12 09:24:26       AMD         25      114        AMD   \n",
       "1  2020-07-12 08:16:45       AMD       1719        1        AMD   \n",
       "2  2020-07-12 08:11:41       AMD         69      135        AMD   \n",
       "3  2020-07-12 02:22:50       AMD         34      155        AMD   \n",
       "4  2020-07-11 23:58:44       AMD        802      730        AMD   \n",
       "\n",
       "          location  retweet_count      screenname search_query  \\\n",
       "0   United Kingdom            0.0   moffphcgaming         #AMD   \n",
       "1     digitalocean            1.0     LinuxDreams         #AMD   \n",
       "2        Amsterdam            1.0    LinuxReviews         #AMD   \n",
       "3    San Francisco            0.0     NdrewGarcia         #AMD   \n",
       "4  New Jersey, USA            0.0  Roger_Clinton1         #AMD   \n",
       "\n",
       "                                                text    twitter_id  \\\n",
       "0  Been on holiday so back now. Gonna try get som...  1.282244e+18   \n",
       "1  RT @LinuxReviews: #Linux architect Linus Torva...  1.282227e+18   \n",
       "2  #Linux architect Linus Torvalds: AVX512 Is \"A ...  1.282226e+18   \n",
       "3  #AMD stuck in a range box chart https://t.co/5...  1.282138e+18   \n",
       "4  $AMD Epyc Milan Leak  Three early Genesis sam...  1.282102e+18   \n",
       "\n",
       "               username  polarity partition_0 partition_1  sentiment  \n",
       "0      MoffPHC Gaming   -0.3102  Technology         AMD          0  \n",
       "1           LinuxDreams   -0.3612  Technology         AMD          0  \n",
       "2          LinuxReviews   -0.3612  Technology         AMD          0  \n",
       "3            Encino_Man   -0.2500  Technology         AMD          0  \n",
       "4  Roger Ocasio-Clinton   -0.3400  Technology         AMD          0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a sample from the data with equally distributed sentiment labels\n",
    "\n",
    "df_tweets_bigtech_negative = df_tweets_bigtech[df_tweets_bigtech[\"sentiment\"]==0]\n",
    "#negative_sample = df_tweets_bigtech_negative.sample(n=3500) \n",
    "# random sampling leads to error in predicting\n",
    "negative_sample = df_tweets_bigtech_negative[:3500]\n",
    "\n",
    "\n",
    "df_tweets_bigtech_neutral = df_tweets_bigtech[df_tweets_bigtech[\"sentiment\"]==1]\n",
    "neutral_sample = df_tweets_bigtech_neutral[:2500]\n",
    "\n",
    "df_tweets_bigtech_positive = df_tweets_bigtech[df_tweets_bigtech[\"sentiment\"]==2]\n",
    "positive_sample = df_tweets_bigtech_positive[:3500]\n",
    "\n",
    "df_tweets_bigtech_10ksample = pd.concat([negative_sample, neutral_sample, positive_sample], axis=0, ignore_index=True)\n",
    "df_tweets_bigtech_10ksample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    3500\n",
       "2    3500\n",
       "1    2500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_bigtech_10ksample[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_bigtech_10ksample[\"sentiment_prediction_finetuned\"] = df_tweets_bigtech_10ksample[\"text\"].apply(lambda x: sentiment_score(x[:500], tokenizer2, model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>file_name</th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "      <th>group_name</th>\n",
       "      <th>location</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>screenname</th>\n",
       "      <th>search_query</th>\n",
       "      <th>text</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>username</th>\n",
       "      <th>polarity</th>\n",
       "      <th>partition_0</th>\n",
       "      <th>partition_1</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_prediction_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-12 09:24:26</td>\n",
       "      <td>AMD</td>\n",
       "      <td>25</td>\n",
       "      <td>114</td>\n",
       "      <td>AMD</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>moffphcgaming</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>Been on holiday so back now. Gonna try get som...</td>\n",
       "      <td>1.282244e+18</td>\n",
       "      <td>MoffPHC Gaming</td>\n",
       "      <td>-0.3102</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-12 08:16:45</td>\n",
       "      <td>AMD</td>\n",
       "      <td>1719</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD</td>\n",
       "      <td>digitalocean</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>RT @LinuxReviews: #Linux architect Linus Torva...</td>\n",
       "      <td>1.282227e+18</td>\n",
       "      <td>LinuxDreams</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-12 08:11:41</td>\n",
       "      <td>AMD</td>\n",
       "      <td>69</td>\n",
       "      <td>135</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>#Linux architect Linus Torvalds: AVX512 Is \"A ...</td>\n",
       "      <td>1.282226e+18</td>\n",
       "      <td>LinuxReviews</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-12 02:22:50</td>\n",
       "      <td>AMD</td>\n",
       "      <td>34</td>\n",
       "      <td>155</td>\n",
       "      <td>AMD</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NdrewGarcia</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>#AMD stuck in a range box chart https://t.co/5...</td>\n",
       "      <td>1.282138e+18</td>\n",
       "      <td>Encino_Man</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-11 23:58:44</td>\n",
       "      <td>AMD</td>\n",
       "      <td>802</td>\n",
       "      <td>730</td>\n",
       "      <td>AMD</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Roger_Clinton1</td>\n",
       "      <td>#AMD</td>\n",
       "      <td>$AMD Epyc Milan Leak  Three early Genesis sam...</td>\n",
       "      <td>1.282102e+18</td>\n",
       "      <td>Roger Ocasio-Clinton</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>Technology</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at file_name  followers  friends group_name  \\\n",
       "0  2020-07-12 09:24:26       AMD         25      114        AMD   \n",
       "1  2020-07-12 08:16:45       AMD       1719        1        AMD   \n",
       "2  2020-07-12 08:11:41       AMD         69      135        AMD   \n",
       "3  2020-07-12 02:22:50       AMD         34      155        AMD   \n",
       "4  2020-07-11 23:58:44       AMD        802      730        AMD   \n",
       "\n",
       "          location  retweet_count      screenname search_query  \\\n",
       "0   United Kingdom            0.0   moffphcgaming         #AMD   \n",
       "1     digitalocean            1.0     LinuxDreams         #AMD   \n",
       "2        Amsterdam            1.0    LinuxReviews         #AMD   \n",
       "3    San Francisco            0.0     NdrewGarcia         #AMD   \n",
       "4  New Jersey, USA            0.0  Roger_Clinton1         #AMD   \n",
       "\n",
       "                                                text    twitter_id  \\\n",
       "0  Been on holiday so back now. Gonna try get som...  1.282244e+18   \n",
       "1  RT @LinuxReviews: #Linux architect Linus Torva...  1.282227e+18   \n",
       "2  #Linux architect Linus Torvalds: AVX512 Is \"A ...  1.282226e+18   \n",
       "3  #AMD stuck in a range box chart https://t.co/5...  1.282138e+18   \n",
       "4  $AMD Epyc Milan Leak  Three early Genesis sam...  1.282102e+18   \n",
       "\n",
       "               username  polarity partition_0 partition_1  sentiment  \\\n",
       "0      MoffPHC Gaming   -0.3102  Technology         AMD          0   \n",
       "1           LinuxDreams   -0.3612  Technology         AMD          0   \n",
       "2          LinuxReviews   -0.3612  Technology         AMD          0   \n",
       "3            Encino_Man   -0.2500  Technology         AMD          0   \n",
       "4  Roger Ocasio-Clinton   -0.3400  Technology         AMD          0   \n",
       "\n",
       "   sentiment_prediction_finetuned  \n",
       "0                               2  \n",
       "1                               0  \n",
       "2                               0  \n",
       "3                               0  \n",
       "4                               2  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_bigtech_10ksample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5633684210526316\n",
      "Precision: 0.5890056459480304\n",
      "Recall: 0.5633684210526316\n",
      "F1-Score: 0.48415694314369734\n"
     ]
    }
   ],
   "source": [
    "performance_metrics(df_tweets_bigtech_10ksample, \"sentiment\", \"sentiment_prediction_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison with not fine-tuned model on tweets-bigtech-data\n",
    "df_tweets_bigtech_10ksample[\"sentiment_prediction\"] = df_tweets_bigtech_10ksample[\"text\"].apply(lambda x: sentiment_score(x[:500], tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5675789473684211\n",
      "Precision: 0.6583776609118885\n",
      "Recall: 0.5675789473684211\n",
      "F1-Score: 0.5742206759300184\n"
     ]
    }
   ],
   "source": [
    "performance_metrics(df_tweets_bigtech_10ksample, \"sentiment\", \"sentiment_prediction\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
